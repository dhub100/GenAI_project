{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from advanced_rag import AdvancedRAG, EmbeddingModelType, ChainType\n",
    "from basic_rag import answer_query as basic_answer_query\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def embedding_model_evaluate_answer(pred: str, gold: str, semantic_model) -> float:\n",
    "    emb_pred = semantic_model.encode(pred, convert_to_tensor=True)\n",
    "    emb_gold = semantic_model.encode(gold, convert_to_tensor=True)\n",
    "    sim = util.cos_sim(emb_pred, emb_gold).item()\n",
    "    return sim\n",
    "\n",
    "def llm_evaluate_answer(pred: str, gold: str, llm, llm_prompt_list: list) -> float:\n",
    "    sim_score = 0\n",
    "    for prompt in llm_prompt_list:\n",
    "        s = llm.get_direct_llm_answer(prompt.replace(\"{gold_answer}\", gold).replace(\"{predicted_answer}\", pred))\n",
    "        try:\n",
    "            s = float(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "        try:\n",
    "            sim_score += s\n",
    "        except:\n",
    "            return np.nan\n",
    "    sim_score /= len(llm_prompt_list)\n",
    "    return sim_score\n",
    "\n",
    "def compute_accuracy(results_df, group_col):\n",
    "    metrics = results_df.groupby(group_col)[\"Correct\"].mean().reset_index()\n",
    "    metrics.columns = [group_col, \"Accuracy\"]\n",
    "    return metrics\n",
    "\n",
    "def evaluate_rag(rag_query_fn, questions_df, semantic_model, self_llm, llm_prompts_list, direct_llm=False):\n",
    "    results = []\n",
    "    for _, row in questions_df.iterrows():\n",
    "        q, gold, category, no = row[\"Question\"], row[\"Answer\"], row[\"Category\"], row[\"no\"]\n",
    "        if direct_llm:\n",
    "            q = \"Please answer based on the book '1984' written by George Orwell. \" + q\n",
    "        pred = rag_query_fn(q)\n",
    "        embeddings_models_score = embedding_model_evaluate_answer(pred, gold, semantic_model)\n",
    "        llm_score = llm_evaluate_answer(pred, gold, self_llm, llm_prompts_list)\n",
    "        results.append({\n",
    "            \"question_no\": no,\n",
    "            \"Predicted Answer\": pred,\n",
    "            \"Category\": category,\n",
    "            \"embeddings_models_score\": embeddings_models_score,\n",
    "            \"llm_score\": llm_score\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def compute_metrics(results_df):\n",
    "    category_acc = compute_accuracy(results_df, \"Category\")\n",
    "\n",
    "    print(\"\\n=== Overall Accuracy ===\")\n",
    "    print(results_df[\"Correct\"].mean())\n",
    "\n",
    "    print(\"\\n=== Accuracy by Category ===\")\n",
    "    print(category_acc)"
   ],
   "id": "a116d7a8c3a214fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "General constants",
   "id": "8ef75a987bc5ffe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "self_llm = AdvancedRAG()\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "questions_df = pd.read_json(\"evaluation/1984_test_questions.json\")\n",
    "\n",
    "llm_prompts_list = [\n",
    "    \"Please score the level of correctness of the predicted answer based on the gold answer. You should give a high score to an answer that contains at least the elements contained in the gold answer. If a few additionnary elements are present in the predicted answer, you can still give a high score for the answer. If elements of the predicted answer are contradicting the gold answer, the final score should be low. 1 is a perfect score (all elements a√¥f the gold answer a in the predicted answer). O is a very low score (None of the elements of the gold answer are in the predicted answer).\\n Gold answer : {gold_answer}. Predicted answer : {predicted_answer}. Please only give your score with no comments, explanations, side notes or post scriptum. Please do not write anything else than th score. (for example: '0.7').\",\n",
    "\n",
    "    \"Please score the level of correctness of the predicted answer based on the gold answer. You should give a high score to an answer that contains at least the elements contained in the gold answer. Do not penalize additionnary elements that are present in the answer but not in the answer. If elements of the predicted answer are contradicting the gold answer, the final score should be low. 1 is a perfect score (all elements of the gold answer a in the predicted answer). O is a very low score (None of the elements of the gold answer are in the predicted answer).\\n Gold answer : {gold_answer}. Predicted answer : {predicted_answer}. Please only give your score with no comments, explanations, side notes or post scriptum. Please do not write anything else than th score. (for example: '0.7').\"\n",
    "]"
   ],
   "id": "3a4711cf5d4b48e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate basic RAG",
   "id": "6807efd40af5bd3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df = evaluate_rag(rag_query_fn=basic_answer_query, questions_df=questions_df.copy(), semantic_model=semantic_model, self_llm=self_llm, llm_prompts_list=llm_prompts_list)\n",
    "\n",
    "results_df.to_json(f\"evaluation/basic_rag_eval_results.json\", orient=\"records\", indent=2)"
   ],
   "id": "f4aea866d0eee10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate direct LLM",
   "id": "6bab51711853ea0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df = evaluate_rag(rag_query_fn=AdvancedRAG().get_direct_llm_answer, questions_df=questions_df.copy(), semantic_model=semantic_model, self_llm=self_llm, llm_prompts_list=llm_prompts_list, direct_llm=True)\n",
    "\n",
    "results_df.to_json(f\"evaluation/directLLM_eval_results.json\", orient=\"records\", indent=2)"
   ],
   "id": "47764ffd41840263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate Advanced RAG with various parameters",
   "id": "2c4564aac3f7fb54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for temperature in [0.1, 0.2, 0.8, 1.2]:\n",
    "    for embedding_model in [EmbeddingModelType.HuggingFace, EmbeddingModelType.Ollama]:\n",
    "        for chain_type in [ChainType.STUFF, ChainType.REFINE, ChainType.MAP_REDUCE]:\n",
    "            for compression in [True, False]:\n",
    "                for nb_chunks in [1, 3, 5]:\n",
    "                    for prompt_type in []:\n",
    "                        print(\"---------------------------------------\")\n",
    "                        print(time.now())\n",
    "                        print(temperature, embedding_model.value, chain_type.value, compression, nb_chunks)\n",
    "                        Advanced_rag = AdvancedRAG(compression=compression,\n",
    "                                                   embedding_model_type=embedding_model,\n",
    "                                                   chain_type=chain_type,\n",
    "                                                   nb_chunks=nb_chunks,\n",
    "                                                   llm_temperature=temperature)\n",
    "                        try:\n",
    "                            results_df = evaluate_rag(rag_query_fn=Advanced_rag.answer_query,\n",
    "                                                      questions_df=questions_df.copy(),\n",
    "                                                      semantic_model=semantic_model,\n",
    "                                                      self_llm=self_llm,\n",
    "                                                      llm_prompts_list=llm_prompts_list)\n",
    "\n",
    "                            results_df[\"compression\"] = compression\n",
    "                            results_df[\"embedding_model\"] = embedding_model.value\n",
    "                            results_df[\"chain_type\"] = chain_type.value\n",
    "                            results_df[\"nb_chunks\"] = nb_chunks\n",
    "                            results_df[\"temperature\"] = temperature\n",
    "\n",
    "                            results_df.to_json(f\"evaluation/advanced_rag_compression_{str(compression)}_embModel_{embedding_model.value}_chainType_{chain_type.value}_nbChunks_{str(nb_chunks)}_LLMtemp_{str(temperature)}_eval_results.json\", orient=\"records\", indent=2)\n",
    "                        except Exception as e:\n",
    "                            print(e)"
   ],
   "id": "6fbb2c39c2bef70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for file in os.listdir(\"evaluation/\"):\n",
    "    if \"test_questions\" in file or file.endswith(\".png\"):\n",
    "        continue\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open(\"evaluation/directLLM_eval_results.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Group scores by category\n",
    "    categories = defaultdict(lambda: {\"embedding\": [], \"llm\": []})\n",
    "\n",
    "    for item in data:\n",
    "        cat = item[\"Category\"]\n",
    "        categories[cat][\"embedding\"].append(item[\"embeddings_models_score\"])\n",
    "        categories[cat][\"llm\"].append(item[\"llm_score\"])\n",
    "\n",
    "    # Prepare plotting data\n",
    "    cat_names = list(categories.keys())\n",
    "\n",
    "    embedding_scores = [vals[\"embedding\"] for vals in categories.values()]\n",
    "    llm_scores = [vals[\"llm\"] for vals in categories.values()]\n",
    "\n",
    "    # Add mean as the last value per category\n",
    "    embedding_with_means = [scores + [np.mean(scores)] for scores in embedding_scores]\n",
    "    llm_with_means = [scores + [np.mean(scores)] for scores in llm_scores]\n",
    "\n",
    "    # Plot setup\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "    # Define colors\n",
    "    color_individual = \"C0\"\n",
    "    color_mean = \"C3\"\n",
    "\n",
    "    for ax, scores, title in zip(\n",
    "        axes, [embedding_with_means, llm_with_means],\n",
    "        [\"Embedding Scoring Method\", \"LLM Comparison Scoring Method\"]\n",
    "    ):\n",
    "        bar_width = 0.1\n",
    "        x = np.arange(len(cat_names))\n",
    "\n",
    "        for i, cat_scores in enumerate(scores):\n",
    "            offsets = np.linspace(-bar_width*(len(cat_scores)-1)/2,\n",
    "                                  bar_width*(len(cat_scores)-1)/2, len(cat_scores))\n",
    "            for j, val in enumerate(cat_scores):\n",
    "                color = color_mean if j == len(cat_scores)-1 else color_individual\n",
    "                bar = ax.bar(x[i] + offsets[j], val, width=bar_width, color=color)\n",
    "                # Add label on top of bar\n",
    "                ax.text(\n",
    "                    x[i] + offsets[j],\n",
    "                    val + 0.02,\n",
    "                    f\"{val:.2f}\",\n",
    "                    ha='center',\n",
    "                    va='bottom',\n",
    "                    fontsize=8,\n",
    "                    rotation=90\n",
    "                )\n",
    "\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Add legend using proxy artists with correct colors\n",
    "    ind_bar = plt.Rectangle((0, 0), 1, 1, color=color_individual)\n",
    "    mean_bar = plt.Rectangle((0, 0), 1, 1, color=color_mean)\n",
    "    axes[0].legend([ind_bar, mean_bar], [\"Individual Scores\", \"Mean\"], loc=\"upper right\")\n",
    "\n",
    "    # Common X labels\n",
    "    axes[-1].set_xticks(np.arange(len(cat_names)))\n",
    "    axes[-1].set_xticklabels(cat_names, rotation=30, ha=\"right\")\n",
    "\n",
    "    plt.suptitle(f\"RAG evaluation results for {file.split('_eval_results.json')[0]}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"evaluation/{file.split('_eval_results.json')[0]}.png\")\n"
   ],
   "id": "30f300ddc20d2388",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
