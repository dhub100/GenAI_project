{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from advanced_rag import answer_query, llm\n",
    "from basic_rag import answer_query as basic_answer_query\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def is_correct(pred: str, gold: str, semantic_model, threshold: float = 0.65) -> bool:\n",
    "    emb_pred = semantic_model.encode(pred, convert_to_tensor=True)\n",
    "    emb_gold = semantic_model.encode(gold, convert_to_tensor=True)\n",
    "    sim = util.cos_sim(emb_pred, emb_gold).item()\n",
    "    return sim >= threshold\n",
    "\n",
    "def compute_accuracy(results_df, group_col):\n",
    "    metrics = results_df.groupby(group_col)[\"Correct\"].mean().reset_index()\n",
    "    metrics.columns = [group_col, \"Accuracy\"]\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_rag(rag_query_fn, questions_df, semantic_model):\n",
    "    results = []\n",
    "    for _, row in questions_df.iterrows():\n",
    "        q, gold, category, trick, difficulty = row[\"Question\"], row[\"Answer\"], row[\"Category\"], row[\"IsTrick\"], row[\"Difficulty\"]\n",
    "        pred = rag_query_fn(q)\n",
    "        correct = is_correct(pred, gold, semantic_model)\n",
    "        results.append({\n",
    "            \"Question\": q,\n",
    "            \"Gold Answer\": gold,\n",
    "            \"Predicted Answer\": pred,\n",
    "            \"Category\": category,\n",
    "            \"Difficulty\": difficulty,\n",
    "            \"IsTrick\": trick,\n",
    "            \"Correct\": correct\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def compute_metrics(results_df):\n",
    "    category_acc = compute_accuracy(results_df, \"Category\")\n",
    "    difficulty_acc = compute_accuracy(results_df, \"Difficulty\")\n",
    "\n",
    "    print(\"\\n=== Overall Accuracy ===\")\n",
    "    print(results_df[\"Correct\"].mean())\n",
    "\n",
    "    print(\"\\n=== Accuracy by Category ===\")\n",
    "    print(category_acc)\n",
    "\n",
    "    print(\"\\n=== Accuracy by Difficulty ===\")\n",
    "    print(difficulty_acc)"
   ],
   "id": "a116d7a8c3a214fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "eval_questions_path = \"evaluation/1984_test_questions.json\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for rag_name, rag in zip([\"basic_rag\", \"advanced_rag\", \"llm\"],[basic_answer_query, answer_query, llm.invoke]):\n",
    "\n",
    "    df = pd.read_json(eval_questions_path)\n",
    "\n",
    "    results_df = evaluate_rag(rag_query_fn=rag, questions_df=df, semantic_model=model)\n",
    "\n",
    "    results_df.to_json(f\"evaluation/{rag_name}_eval_results.json\", orient=\"records\", indent=2)"
   ],
   "id": "307a9eb1722b068c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
