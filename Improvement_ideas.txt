Fixed Chunking → Semantic Chunking:
Currently, the document is split into chunks of fixed length (e.g., 2000 characters) without considering sentence or paragraph structure.
We could apply semantic chunking, splitting based on meaning or text structure, to ensure that each chunk contains a coherent piece of information.

Static Retrieval -> Query Expansion:
Currently, the RAG always retrieves a fixed number of chunks of fixed length.
We could add query expansion so an LLM can reformulate the question and retrieve more relevant chunks.

Raw Context -> Context Compression:
Currently, full retrieved chunks are passed to the LLM without any optimization.
We could add a context compression step — summarize or clean the retrieved text before sending it to the LLM.

Simple Prompting -> Structured Prompting:
Currently, the prompt sent to the LLM has no clear structure (mixing the user question and RAG context).
We could implement a structured prompt with separate sections for System, Context, and Question to ensure more reliable responses.


Regarding references to the document source, three steps are required:

- Store metadata together with each chunk in the vector database.
- Ensure the metadata is retrieved along with the text and passed to the LLM.
- Add to the user prompt an instruction, e.g. “Include the document name and page number in parentheses.”

Possible things to test and check how performance is affected:

1. Use different embedders: e.g "nomic-embed-text" from ollama

#from langchain_ollama import OllamaEmbeddings

#embeddings = OllamaEmbeddings(model="nomic-embed-text")
#vectorstore = FAISS.from_documents(texts, embeddings)

2. Test other chain types than "stuff" => such as "map_reduce" (sends each junk separately to the llm then summarizes results)
or "refine" (sends each junk individually to the llm)

3. we could also play around with the numbers of chunks retrieved to see how the result is influenced:
e.g retriever = vectorstore.as_retriever(search_kwargs={"k": 6}


