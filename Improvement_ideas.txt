Fixed Chunking → Semantic Chunking:
Currently, the document is split into chunks of fixed length (e.g., 2000 characters) without considering sentence or paragraph structure.
We could apply semantic chunking, splitting based on meaning or text structure, to ensure that each chunk contains a coherent piece of information.

Static Retrieval -> Query Expansion:
Currently, the RAG always retrieves a fixed number of chunks of fixed length.
We could add query expansion so an LLM can reformulate the question and retrieve more relevant chunks.

Raw Context -> Context Compression:
Currently, full retrieved chunks are passed to the LLM without any optimization.
We could add a context compression step — summarize or clean the retrieved text before sending it to the LLM.

Simple Prompting -> Structured Prompting:
Currently, the prompt sent to the LLM has no clear structure (mixing the user question and RAG context).
We could implement a structured prompt with separate sections for System, Context, and Question to ensure more reliable responses.


Regarding references to the document source, three steps are required:

- Store metadata together with each chunk in the vector database.
- Ensure the metadata is retrieved along with the text and passed to the LLM.
- Add to the user prompt an instruction, e.g. “Include the document name and page number in parentheses.”
