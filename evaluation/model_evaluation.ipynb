{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from RAGs.advanced_rag import AdvancedRAG, EmbeddingModelType, ChainType\n",
    "from RAG_Database.prompts import PROMPT_PRESETS\n",
    "from RAGs.basic_rag import answer_query as basic_answer_query\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datetime import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def embedding_model_evaluate_answer(pred: str, gold: str, semantic_model) -> float:\n",
    "    emb_pred = semantic_model.encode(pred, convert_to_tensor=True)\n",
    "    emb_gold = semantic_model.encode(gold, convert_to_tensor=True)\n",
    "    sim = util.cos_sim(emb_pred, emb_gold).item()\n",
    "    return sim\n",
    "\n",
    "def llm_evaluate_answer(pred: str, gold: str, llm, llm_prompt_list: list) -> float:\n",
    "    sim_score = 0\n",
    "    for prompt in llm_prompt_list:\n",
    "        s = llm.get_direct_llm_answer(prompt.replace(\"{gold_answer}\", gold).replace(\"{predicted_answer}\", pred))\n",
    "        try:\n",
    "            s = float(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "        try:\n",
    "            sim_score += s\n",
    "        except:\n",
    "            return np.nan\n",
    "    sim_score /= len(llm_prompt_list)\n",
    "    return sim_score\n",
    "\n",
    "def compute_accuracy(results_df, group_col):\n",
    "    metrics = results_df.groupby(group_col)[\"Correct\"].mean().reset_index()\n",
    "    metrics.columns = [group_col, \"Accuracy\"]\n",
    "    return metrics\n",
    "\n",
    "def evaluate_rag(rag_query_fn, questions_df, semantic_model, self_llm, llm_prompts_list, direct_llm=False):\n",
    "    results = []\n",
    "    for _, row in questions_df.iterrows():\n",
    "        q, gold, category, no = row[\"Question\"], row[\"Answer\"], row[\"Category\"], row[\"no\"]\n",
    "        if direct_llm:\n",
    "            q = \"Please answer based on the book '1984' written by George Orwell. \" + q\n",
    "        pred = rag_query_fn(q)\n",
    "        print(pred.split(\"Sources: \")[0])\n",
    "        embeddings_models_score = embedding_model_evaluate_answer(pred.split(\"Sources: \")[0], gold, semantic_model)\n",
    "        llm_score = llm_evaluate_answer(pred.split(\"Sources: \")[0], gold, self_llm, llm_prompts_list)\n",
    "        results.append({\n",
    "            \"question_no\": no,\n",
    "            \"Predicted Answer\": pred,\n",
    "            \"Category\": category,\n",
    "            \"embeddings_models_score\": embeddings_models_score,\n",
    "            \"llm_score\": llm_score\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def create_specific_rag_eval_figure(results_path, figure_path):\n",
    "    for file in os.listdir(results_path):\n",
    "        # Load the JSON data\n",
    "        with open(f\"{results_path}/{file}\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Group scores by category\n",
    "        categories = defaultdict(lambda: {\"embedding\": [], \"llm\": []})\n",
    "\n",
    "        for item in data:\n",
    "            cat = item[\"Category\"]\n",
    "\n",
    "            if item[\"embeddings_models_score\"] is not None:\n",
    "                categories[cat][\"embedding\"].append(item[\"embeddings_models_score\"])\n",
    "\n",
    "            if item[\"llm_score\"] is not None:\n",
    "                categories[cat][\"llm\"].append(item[\"llm_score\"])\n",
    "\n",
    "        # Prepare plotting data\n",
    "        cat_names = list(categories.keys())\n",
    "\n",
    "        embedding_scores = [vals[\"embedding\"] for vals in categories.values()]\n",
    "        llm_scores = [vals[\"llm\"] for vals in categories.values()]\n",
    "\n",
    "        # Add mean as the last value per category\n",
    "        embedding_with_means = [scores + [np.mean(scores)] for scores in embedding_scores]\n",
    "        llm_with_means = [scores + [np.mean(scores)] for scores in llm_scores]\n",
    "\n",
    "        # Plot setup\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "        # Define colors\n",
    "        color_individual = \"C0\"\n",
    "        color_mean = \"C3\"\n",
    "\n",
    "        for ax, scores, title in zip(\n",
    "            axes, [embedding_with_means, llm_with_means],\n",
    "            [\"Embedding Scoring Method\", \"LLM Comparison Scoring Method\"]\n",
    "            ):\n",
    "            bar_width = 0.1\n",
    "            x = np.arange(len(cat_names))\n",
    "\n",
    "            for i, cat_scores in enumerate(scores):\n",
    "                offsets = np.linspace(-bar_width*(len(cat_scores)-1)/2,\n",
    "                                      bar_width*(len(cat_scores)-1)/2, len(cat_scores))\n",
    "                for j, val in enumerate(cat_scores):\n",
    "                    color = color_mean if j == len(cat_scores)-1 else color_individual\n",
    "                    bar = ax.bar(x[i] + offsets[j], val, width=bar_width, color=color)\n",
    "\n",
    "                    ax.text(\n",
    "                        x[i] + offsets[j],\n",
    "                        val + 0.02,\n",
    "                        f\"{val:.2f}\",\n",
    "                        ha='center',\n",
    "                        va='bottom',\n",
    "                        fontsize=8,\n",
    "                        rotation=90\n",
    "                    )\n",
    "\n",
    "            ax.set_title(title, fontsize=14)\n",
    "            ax.set_ylabel(\"Score\")\n",
    "            ax.set_ylim(0, 1.1)\n",
    "            ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Add legend using proxy artists with correct colors\n",
    "        ind_bar = plt.Rectangle((0, 0), 1, 1, color=color_individual)\n",
    "        mean_bar = plt.Rectangle((0, 0), 1, 1, color=color_mean)\n",
    "        axes[0].legend([ind_bar, mean_bar], [\"Individual Scores\", \"Mean\"], loc=\"upper right\")\n",
    "\n",
    "        # Common X labels\n",
    "        axes[-1].set_xticks(np.arange(len(cat_names)))\n",
    "        axes[-1].set_xticklabels(cat_names, rotation=30, ha=\"right\")\n",
    "\n",
    "        if len(file) > 50:\n",
    "            title = f\"RAG evaluation results for \\n{file.split('_eval_results.json')[0].split(\"rag_\")[1]}\"\n",
    "            title = \"\\n_nb_chunks\".join(title.split(\"_nbChunks\"))\n",
    "        else:\n",
    "            title = file\n",
    "        plt.suptitle(title)\n",
    "        plt.savefig(f\"{figure_path}/{file.split('_eval_results.json')[0]}.png\")\n",
    "        plt.close(\"all\")\n",
    "\n",
    "def plot_subplots(data: pd.DataFrame, group_label: str, mode: str, save_path: str):\n",
    "    \"\"\"Create multi-subplot figure: vertical boxplots, one per parameter.\"\"\"\n",
    "    n_params = len(parameters)\n",
    "    n_cols = 3\n",
    "    n_rows = int((n_params + n_cols - 1) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, param in enumerate(parameters):\n",
    "        sns.boxplot(\n",
    "            data=data,\n",
    "            x=param,\n",
    "            y=\"llm_score\",\n",
    "            hue=param,\n",
    "            ax=axes[i],\n",
    "            palette=\"Set2\"\n",
    "        )\n",
    "        axes[i].set_title(param)\n",
    "        axes[i].set_xlabel(param)\n",
    "        axes[i].set_ylabel(\"LLM Score\")\n",
    "        #axes[i].legend(title=param, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(f\"LLM Score Distributions — {mode.capitalize()}: {group_label}\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ],
   "id": "a116d7a8c3a214fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "General constants",
   "id": "8ef75a987bc5ffe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "PWD = os.environ.get(\"PROJECT_WORKING_DIRECTORY\")\n",
    "self_llm = AdvancedRAG()\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "questions_df = pd.read_json(f\"{PWD}/evaluation/1984_test_questions.json\")\n",
    "\n",
    "llm_prompts_list = [\n",
    "    \"Please score the level of correctness of the predicted answer based on the gold answer. You should give a high score to an answer that contains at least the elements contained in the gold answer. If a few additionnary elements are present in the predicted answer, you can still give a high score for the answer. If elements of the predicted answer are contradicting the gold answer, the final score should be low. 1 is a perfect score (all elements aôf the gold answer a in the predicted answer). O is a very low score (None of the elements of the gold answer are in the predicted answer).\\n Gold answer : {gold_answer}. Predicted answer : {predicted_answer}. Please only give your score with no comments, explanations, side notes or post scriptum. Please do not write anything else than th score. (for example: '0.7').\",\n",
    "\n",
    "    \"Please score the level of correctness of the predicted answer based on the gold answer. You should give a high score to an answer that contains at least the elements contained in the gold answer. Do not penalize additionnary elements that are present in the answer but not in the answer. If elements of the predicted answer are contradicting the gold answer, the final score should be low. 1 is a perfect score (all elements of the gold answer a in the predicted answer). O is a very low score (None of the elements of the gold answer are in the predicted answer).\\n Gold answer : {gold_answer}. Predicted answer : {predicted_answer}. Please only give your score with no comments, explanations, side notes or post scriptum. Please do not write anything else than th score. (for example: '0.7').\"\n",
    "]"
   ],
   "id": "3a4711cf5d4b48e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate basic RAG",
   "id": "6807efd40af5bd3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df = evaluate_rag(rag_query_fn=basic_answer_query, questions_df=questions_df.copy(), semantic_model=semantic_model, self_llm=self_llm, llm_prompts_list=llm_prompts_list)\n",
    "\n",
    "results_df.to_json(f\"{PWD}/evaluation/basicRAG_eval_results/basic_rag_eval_results.json\", orient=\"records\", indent=2)"
   ],
   "id": "f4aea866d0eee10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "create_specific_rag_eval_figure(results_path=f\"{PWD}/evaluation/basicRAG_eval_results\", figure_path=f\"{PWD}/evaluation/basicRAG_eval_results_figures\")",
   "id": "223c11afb9b7ce6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate direct LLM",
   "id": "6bab51711853ea0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df = evaluate_rag(rag_query_fn=AdvancedRAG().get_direct_llm_answer, questions_df=questions_df.copy(), semantic_model=semantic_model, self_llm=self_llm, llm_prompts_list=llm_prompts_list, direct_llm=True)\n",
    "\n",
    "results_df.to_json(f\"{PWD}/evaluation/directLLM_eval_results/directLLM_eval_results.json\", orient=\"records\", indent=2)"
   ],
   "id": "47764ffd41840263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "create_specific_rag_eval_figure(results_path=f\"{PWD}/evaluation/directLLM_eval_results\", figure_path=f\"{PWD}/evaluation/directLLM_eval_results_figures\")",
   "id": "59b93c90d5cf42ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate Advanced RAG with various parameters",
   "id": "2c4564aac3f7fb54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for temperature in [0.1, 0.2, 0.8, 1.2]:\n",
    "    for embedding_model in [EmbeddingModelType.HuggingFace, EmbeddingModelType.Ollama]:\n",
    "        for chain_type in [ChainType.STUFF, ChainType.REFINE, ChainType.MAP_REDUCE]:\n",
    "            for compression in [True, False]:\n",
    "                for nb_chunks in [1, 3, 5]:\n",
    "                    for role_name in [\"default\", \"academic\", \"debate\", \"psychology\"]:\n",
    "                        print(\"---------------------------------------\")\n",
    "                        print(datetime.now())\n",
    "                        print(temperature, embedding_model.value, chain_type.value, compression, nb_chunks, role_name)\n",
    "                        prompt = PROMPT_PRESETS[role_name]\n",
    "\n",
    "                        Advanced_rag = AdvancedRAG(compression=compression,\n",
    "                                                   embedding_model_type=embedding_model,\n",
    "                                                   chain_type=chain_type,\n",
    "                                                   nb_chunks=nb_chunks,\n",
    "                                                   prompt=prompt,\n",
    "                                                   llm_temperature=temperature)\n",
    "                        try:\n",
    "                            results_df = evaluate_rag(rag_query_fn=Advanced_rag.answer_query,\n",
    "                                                      questions_df=questions_df.copy(),\n",
    "                                                      semantic_model=semantic_model,\n",
    "                                                      self_llm=self_llm,\n",
    "                                                      llm_prompts_list=llm_prompts_list)\n",
    "\n",
    "                            results_df[\"compression\"] = compression\n",
    "                            results_df[\"embedding_model\"] = embedding_model.value\n",
    "                            results_df[\"chain_type\"] = chain_type.value\n",
    "                            results_df[\"nb_chunks\"] = nb_chunks\n",
    "                            results_df[\"role_name\"] = role_name\n",
    "                            results_df[\"temperature\"] = temperature\n",
    "\n",
    "                            results_df.to_json(f\"{PWD}/evaluation/advancedRAG_eval_results/advanced_rag_compression_{str(compression)}_embModel_{embedding_model.value}_chainType_{chain_type.value}_nbChunks_{str(nb_chunks)}_role{role_name}_LLMtemp_{str(temperature)}_eval_results.json\", orient=\"records\", indent=2)\n",
    "                        except Exception as e:\n",
    "                            print(e)"
   ],
   "id": "6fbb2c39c2bef70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Results visualization",
   "id": "dc23928a014af807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "create_specific_rag_eval_figure(results_path=f\"{PWD}/evaluation/advancedRAG_eval_results\", figure_path=f\"{PWD}/evaluation/advancedRAG_eval_results_figures\")",
   "id": "fba35d61ac073c54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_path = f\"{PWD}/evaluation/advancedRAG_eval_results\"\n",
    "files = os.listdir(base_path)\n",
    "dfs = []\n",
    "for f in files:\n",
    "    if \"LLMtemp_0.1\" in f:\n",
    "        with open(os.path.join(base_path, f)) as fp:\n",
    "            try:\n",
    "                df = pd.DataFrame(json.load(fp))\n",
    "                df[\"source_file\"] = f\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {f}: {e}\")\n",
    "all_results = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# merge all results files\n",
    "all_results = all_results.dropna(subset=[\"llm_score\", \"Category\", \"question_no\"])\n",
    "for col in [\"compression\", \"embedding_model\", \"chain_type\", \"nb_chunks\", \"role_name\"]:\n",
    "    all_results[col] = all_results[col].astype(str)\n",
    "\n",
    "parameters = [\"compression\", \"embedding_model\", \"chain_type\", \"nb_chunks\", \"role_name\"]\n",
    "\n",
    "folders = {\n",
    "    \"parameter\": f\"{PWD}/evaluation/advancedRAG_per_parameter_figures\",\n",
    "    \"category\": f\"{PWD}/evaluation/advancedRAG_per_question_category_figures\",\n",
    "    \"question\": f\"{PWD}/evaluation/advancedRAG_per_single_question_figures\",\n",
    "}\n",
    "for p in folders.values():\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "# Per parameter figures\n",
    "# Figures per parameter\n",
    "for param in parameters:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(\n",
    "        data=all_results,\n",
    "        x=\"Category\",\n",
    "        y=\"llm_score\",\n",
    "        hue=param,\n",
    "        palette=\"Set2\"\n",
    "    )\n",
    "    plt.title(f\"LLM Score by Question Category and {param}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Question Category\")\n",
    "    plt.ylabel(\"LLM Score\")\n",
    "    plt.legend(title=param, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(folders[\"parameter\"] + f\"/boxplot_{param}.png\", bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Per question category figures\n",
    "for cat in all_results[\"Category\"].unique():\n",
    "    subset = all_results[all_results[\"Category\"] == cat]\n",
    "    save_path = os.path.join(folders[\"category\"], f\"subplots_category_{cat.replace(' ', '_')}.png\")\n",
    "    plot_subplots(subset, group_label=cat, mode=\"category\", save_path=save_path)\n",
    "\n",
    "# Per single question figures\n",
    "for q in sorted(all_results[\"question_no\"].unique()):\n",
    "    subset = all_results[all_results[\"question_no\"] == q]\n",
    "    cat = subset[\"Category\"].iloc[0] if len(subset) > 0 else \"Unknown\"\n",
    "    save_path = os.path.join(folders[\"question\"], f\"subplots_question_{q}.png\")\n",
    "    plot_subplots(subset, group_label=f\"Q{q} ({cat})\", mode=\"question\", save_path=save_path)"
   ],
   "id": "a6bd7c1fafad7299",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
